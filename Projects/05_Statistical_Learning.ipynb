{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f254a551",
   "metadata": {},
   "source": [
    "# Statistical learning\n",
    "\n",
    "In this project we briefly discuss a few packages to do statistical learning in Julia.\n",
    "\n",
    "*Aside:* I'm not an expert in this field, so if some of you has more insights or better tricks / setups to try, I'm happy for any comments!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-metro",
   "metadata": {},
   "source": [
    "## Stheno - Gaussian process regression\n",
    "\n",
    "This first section considers Gaussian process regression and inference as implemented in the [Stheno](https://juliagaussianprocesses.github.io/Stheno.jl) package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7ea7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "using AbstractGPs\n",
    "using Stheno\n",
    "using Plots\n",
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690830d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some synthetic data\n",
    "N = 50\n",
    "xdata = 2π * rand(N)\n",
    "y = sin.(xdata) + 0.05*randn(N);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focused-tumor",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup zero-mean Gaussian Process with a squared exponential kernel σ² exp(-||x-x'||^2 / 2l²)\n",
    "l  = 0.4\n",
    "σ² = 0.6\n",
    "f  = @gppp let\n",
    "    # Build a Gaussian Process Programming Problem, but only with one GP\n",
    "    # (could add multiple GPs here with parameters that depend on each other etc.)\n",
    "    f1 = σ² * stretch(GP(SEKernel()), 1 / l)\n",
    "end\n",
    "\n",
    "# Sample from the GP f1 (with some IID observation noise)\n",
    "# which is put on the diagonal of the Kernel\n",
    "const x = GPPPInput(:f1, xdata)\n",
    "σ²ₙ = 0.05\n",
    "fx = f(x, σ²ₙ)\n",
    "\n",
    "# Compute posterior over f given data y\n",
    "f_posterior = posterior(fx, y)\n",
    "\n",
    "# Plot data\n",
    "plt = plot();\n",
    "scatter!(plt, x.x, y; color=:red, label=\"\");\n",
    "\n",
    "# Plot posterior\n",
    "x_plot = range(-0.5, 6.7; length=1000);\n",
    "xp = GPPPInput(:f1, x_plot)\n",
    "plot!(plt, x_plot, f_posterior(xp); label=\"\", color=:blue,\n",
    "      fillalpha=0.2, linewidth=2) \n",
    "plot!(\n",
    "    plt, x_plot, rand(f_posterior(xp, 1e-9), 10);\n",
    "    samples=10, markersize=1, alpha=0.3, label=\"\", color=:blue,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-winning",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameter optimisation\n",
    "using Optim\n",
    "using Zygote\n",
    "\n",
    "# Unpack hyperparameters (given in log scale)\n",
    "# Fudge term to avoid nasty zeros ...\n",
    "unpack(θ) = exp.(θ) .+ 1e-6\n",
    "\n",
    "# nlml = negative log marginal likelihood (of θ)\n",
    "function nlml(θ)\n",
    "    l, σ², σ²ₙ = unpack(θ)\n",
    "    f = @gppp let\n",
    "        f1 = σ² * stretch(GP(SEKernel()), 1 / l)\n",
    "    end\n",
    "    -logpdf(f(x, σ²ₙ), y)\n",
    "end\n",
    "\n",
    "# Optimise using BFGS with adjoint-mode autodiff gradient on nlml\n",
    "θ0 = randn(3);\n",
    "results = Optim.optimize(nlml, nlml', θ0, BFGS(); inplace=false)\n",
    "l_opt, σ²_opt, σ²ₙ_opt = unpack(results.minimizer)\n",
    "\n",
    "@show σ²_opt\n",
    "@show l_opt\n",
    "@show σ²ₙ_opt\n",
    "\n",
    "# Optimal GP and posterior\n",
    "f_opt = @gppp let\n",
    "    f1 = σ²_opt * stretch(GP(SEKernel()), 1 / l_opt)\n",
    "end\n",
    "f_posterior_opt = posterior(f(x, σ²ₙ_opt), y)\n",
    "\n",
    "# Add to plot ...\n",
    "plot!(plt, x_plot, f_posterior_opt(xp); label=\"\", color=:orange,\n",
    "      fillalpha=0.2, linewidth=2)\n",
    "plot!(\n",
    "    plt, x_plot, rand(f_posterior(xp, 1e-9), 10);\n",
    "    samples=10, markersize=1, alpha=0.3, label=\"\", color=:orange,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b20ad3",
   "metadata": {},
   "source": [
    "## Flux - The ML library that doesn't make you tensor\n",
    "\n",
    "[Flux](https://fluxml.ai/) has become the de-facto standard for neural-network-based statistical learning in Julia. A good starting point for any NN-based learning problem is the [Flux model zoo](https://github.com/FluxML/model-zoo/).\n",
    "\n",
    "What we will do here is handwriting recognition based on the `MNIST` data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c6d3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n",
    "using CUDA\n",
    "using Plots\n",
    "using PyCall\n",
    "using MLDatasets: MNIST\n",
    "using MLDataPattern\n",
    "using Flux.Data: DataLoader\n",
    "using FluxTraining\n",
    "\n",
    "# \"Accept\" terms when downloading MNIST image data\n",
    "ENV[\"DATADEPS_ALWAYS_ACCEPT\"] = true;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bb05a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load handwritten digits dataset\n",
    "x_data = reshape(MNIST.traintensor(Float32), 28, 28, 1, :)\n",
    "y_data = Flux.onehotbatch(MNIST.trainlabels(), 0:9)\n",
    "\n",
    "# Split into training, validation, testing\n",
    "traindata, valdata, testdata = splitobs((x_data, y_data), at=(0.9, 0.05))\n",
    "\n",
    "# Show sizes\n",
    "@show size(traindata[1]) size(valdata[1]) size(testdata[1])\n",
    "@show size(traindata[2]) size(valdata[2]) size(testdata[2]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aae9d27",
   "metadata": {},
   "source": [
    "Now let us get a quick idea what we are dealing with. We plot a few of the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a0f1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = map(1:25) do i\n",
    "    p = plot(Gray.(1 .- testdata[1][:, :, 1, i])'; yaxis=nothing, xaxis=nothing)\n",
    "    label = Flux.onecold(testdata[2][:, i], 0:9)\n",
    "    title!(p, string(label), titlefontsize=8)\n",
    "end\n",
    "plot(plots...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e82b13b",
   "metadata": {},
   "source": [
    "Ok ... so a bunch of handwritten numbers, labelled with the appropriate image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98e2547",
   "metadata": {},
   "source": [
    "Next we define a convolutional neural network to be trained. Each Image has 28x28 pixels, that's the input size. Output is the 10 probabilities for being one of the numbers 0 to 9.\n",
    "\n",
    "For more details on the layers and models, see\n",
    "- https://fluxml.ai/Flux.jl/stable/models/layers/\n",
    "- https://fluxml.ai/Flux.jl/stable/models/nnlib/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d590c036",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Chain(\n",
    "    Conv((3, 3), 1 => 6, relu),   # Convolution layer, 6 convolution kernels of size 3x3, relu activation\n",
    "    AdaptiveMaxPool((13, 13)),    # In each layer keep only the largest of 2x2 windows\n",
    "    Conv((3, 3), 6 => 16, relu),  # Another convolution ...\n",
    "    AdaptiveMaxPool((5, 5)),      # ... and another pooling layer\n",
    "    flatten,                      # Flatten all the data\n",
    "    Dense(400, 100, relu),        # down to 100 outputs by a dense layer\n",
    "    Dense(100, 84, relu),         # to 84\n",
    "    Dense(84, 10),                # to 10\n",
    "    softmax\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6f1f91",
   "metadata": {},
   "source": [
    "Now we train (This will take a while ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7edaf75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss = Flux.Losses.crossentropy\n",
    "opt  = Flux.Optimise.ADAM()\n",
    "\n",
    "train_iterator = DataLoader(traindata, batchsize=250, shuffle=true)\n",
    "valid_iterator = DataLoader(valdata,   batchsize=250, shuffle=true)\n",
    "learner        = Learner(model, (train_iterator, valid_iterator),\n",
    "                         opt, loss, Metrics(accuracy), ToGPU())  # Use GPU if available\n",
    "\n",
    "nepochs = 10\n",
    "FluxTraining.fit!(learner, nepochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c5ec4c",
   "metadata": {},
   "source": [
    "To verify our training did not overfit, we check the learning rate comparing the training and validation loss over the learning epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36425f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss   = learner.cbstate.metricsepoch[TrainingPhase()][:Loss].values\n",
    "validation_loss = learner.cbstate.metricsepoch[ValidationPhase()][:Loss].values\n",
    "\n",
    "p = plot(training_loss, label=\"Training loss\")\n",
    "p = plot!(p, validation_loss, label=\"Validation loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f6ceaf",
   "metadata": {},
   "source": [
    "As we see the rates for training and validation are nicely paralell, so we did not yet overfit.\n",
    "\n",
    "Let's assume we are happy with our results (we could do another 10 epochs of training if we we were not). After all training is done we check how good our model is with the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72022c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using the test data\n",
    "test_pred = model(testdata[1])\n",
    "\n",
    "@show loss(test_pred, testdata[2])\n",
    "@show accuracy(test_pred, testdata[2]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9640420d",
   "metadata": {},
   "source": [
    "Not too bad, let's see what this means in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8934a8f3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plots = map(1:25) do i\n",
    "    p = plot(Gray.(1 .- testdata[1][:, :, 1, i])'; yaxis=nothing, xaxis=nothing)\n",
    "    prediction = findmax(vec(model(testdata[1][:, :, :, i:i])))\n",
    "    title!(p, \"$(prediction[2]-1) -> $(round(100prediction[1], digits=1))%\",\n",
    "    titlefontsize=8)\n",
    "end\n",
    "plot(plots...)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
